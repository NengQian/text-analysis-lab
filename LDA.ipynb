{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'BMW-AnnualReport-2010.json', 'BMW-AnnualReport-2011.json', \n",
    "    'BMW-AnnualReport-2012.json','BMW-AnnualReport-2013.json', \n",
    "    'BMW-AnnualReport-2014.json','BMW-AnnualReport-2015.json', \n",
    "    'BMW-AnnualReport-2016.json', 'BMW-AnnualReport-2017.json', \n",
    " \n",
    "    'CarlZeissMeditec-AnnualReport-2011.json',\n",
    "    'CarlZeissMeditec-AnnualReport-2012.json', 'CarlZeissMeditec-AnnualReport-2013.json', \n",
    "    'CarlZeissMeditec-AnnualReport-2014.json','CarlZeissMeditec-AnnualReport-2015.json', \n",
    "    'CarlZeissMeditec-AnnualReport-2016.json', 'CarlZeissMeditec-AnnualReport-2017.json',\n",
    " \n",
    "    'BVB-AnnualReport-2010.json', 'BVB-AnnualReport-2011.json', \n",
    "    'BVB-AnnualReport-2012.json', 'BVB-AnnualReport-2013.json', \n",
    "    'BVB-AnnualReport-2014.json', 'BVB-AnnualReport-2015.json',\n",
    "    'BVB-AnnualReport-2016.json', 'BVB-AnnualReport-2017.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = 'type'\n",
    "PARAGRAPH = 'paragraph'\n",
    "CONTENT = 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readContentOfFile(file_name):\n",
    "    content = ''\n",
    "    try:\n",
    "        with open(file_name) as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                typeDoc = item[TYPE]\n",
    "                if typeDoc == PARAGRAPH:\n",
    "                    content += item[CONTENT]\n",
    "    except:\n",
    "        FileUtils.fix_json(file_name)\n",
    "        with open(file_name) as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                typeDoc = item[TYPE]\n",
    "                if typeDoc == PARAGRAPH:\n",
    "                    content += item[CONTENT]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop = ['million', 'tausend', 'eur', 'teur', '*', '+', '&','%']\n",
    "def perform_lemmatization(document):\n",
    "    content_of_document = readContentOfFile(document)\n",
    "    sentence = nlp(content_of_document)\n",
    "    filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "    filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "    filtered_words_withoutcurrency = [word for word in filtered_words_withoutdigits if not word.is_currency]\n",
    "    filtered_words_withoutverbs = [word for word in filtered_words_withoutcurrency if word.pos_ != 'VERB']\n",
    "    filtered_words_withoutnum = [word for word in filtered_words_withoutverbs if word.pos_ != 'NUM']\n",
    "    filtered_words_withoutsym = [word for word in filtered_words_withoutnum if word.pos_ != 'SYM']\n",
    "    filtered_words_withoutpunc = [word for word in filtered_words_withoutsym if word.pos_ != 'PUNCT']\n",
    "    filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "    filtered_lemmas = [word for word in filtered_lemmas if not word in extra_stop ]\n",
    "    lemmatized_content = \" \".join(item for item in filtered_lemmas)\n",
    "    return lemmatized_content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(FILE_PATH) if isfile(join(FILE_PATH, f))]\n",
    "onlyfiles = onlyfiles[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.35024881362915\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lemm_docs_prep = [ perform_lemmatization(FILE_PATH + document) for document in onlyfiles]\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct vocabulary for BMW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw_lemm_docs_prep = [\n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2010.json'), \n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2011.json'), \n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2012.json'),\n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2013.json'), \n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2014.json'), \n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2015.json'),\n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2016.json'), \n",
    "     perform_lemmatization(FILE_PATH + 'BMW-AnnualReport-2017.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16842913627624512\n"
     ]
    }
   ],
   "source": [
    "vectorizer_bmw = TfidfVectorizer(max_df=0.7)\n",
    "start_time = time.time()\n",
    "tfidf_matrix_bmw = vectorizer_bmw.fit_transform(bmw_lemm_docs_prep)\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw_feature_names = vectorizer_bmw.get_feature_names()\n",
    "bmw_corpus_index = [n for n in [\n",
    "    'BMW-2010', 'BMW-2011', 'BMW-2012', \n",
    "    'BMW-2013', 'BMW-2014', 'BMW-2015',\n",
    "    'BMW-2016', 'BMW-2017']]\n",
    "idf = vectorizer_bmw.idf_\n",
    "df = pd.DataFrame(tfidf_matrix_bmw.T.todense(), index=bmw_feature_names, columns=bmw_corpus_index)\n",
    "df['idf'] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '000er',\n",
       " '000quadratme',\n",
       " '000ste',\n",
       " '000sten',\n",
       " '030',\n",
       " '032',\n",
       " '036',\n",
       " '04',\n",
       " '043']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.sort_values(by=['BMW-2010'], ascending=False)\n",
    "bmw_df = df[(df['idf'] != 1)]\n",
    "bmw_df.head(10).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw_vocab = set()\n",
    "bmw_vocab.update(bmw_df.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: bewertungsmethode: 24.33352 vermietete: 24.33344 concept: 23.33371 icl: 23.33333 amsterdam: 22.33362 citroën: 21.33358 iasb: 21.33353 operate: 19.33348 there: 19.33274 em: 17.33354\n",
      "Topic #1: vehicle: 31.33375 megacity: 28.33372 leistungsindikatoren: 21.33334 nettoschuld: 20.33369 dem: 18.33357 vermietete: 18.33323 ergebnisauswirkungen: 18.33297 zweijährig: 17.33291 überarbeitet: 16.33342 bilanzsumme: 15.33347\n",
      "Topic #2: next: 46.33456 co2: 36.33408 ergebnisauswirkungen: 28.33439 zweijährig: 28.33439 there: 28.33392 here: 24.33384 brexit: 24.33333 digitalisierung: 20.33417 iperformance: 20.33389 chancenbericht: 18.33398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(vocabulary = bmw_vocab, max_df=0.7)\n",
    "tf = tf_vectorizer.fit_transform(bmw_lemm_docs_prep)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3,  max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_with_rep_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.852187871933\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "common_vocabularly_lem = set()\n",
    "for document in onlyfiles:\n",
    "    content_of_document = readContentOfFile(FILE_PATH + document)\n",
    "    sentence = nlp(content_of_document)\n",
    "    # problems with non-lower case stop words\n",
    "    filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "    filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "    filtered_words_withoutcurrency = [word for word in filtered_words_withoutdigits if not word.is_currency]\n",
    "    filtered_words_withoutverbs = [word for word in filtered_words_withoutcurrency if word.pos_ != 'VERB']\n",
    "    filtered_words_withoutnum = [word for word in filtered_words_withoutverbs if word.pos_ != 'NUM']\n",
    "    filtered_words_withoutsym = [word for word in filtered_words_withoutnum if word.pos_ != 'SYM']\n",
    "    filtered_words_withoutpunc = [word for word in filtered_words_withoutsym if word.pos_ != 'PUNCT']\n",
    "    filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "    vocabularly = set()\n",
    "    for word in filtered_lemmas:\n",
    "        vocabularly.add(word.replace('\\n', '').strip().lower())\n",
    "    new_vocab = set()\n",
    "    for u in vocabularly:\n",
    "        if u != '':\n",
    "            new_vocab.add(u)\n",
    "\n",
    "#     lemmatized_content = \" \".join(item for item in filtered_lemmas)\n",
    "#     vectorizer = TfidfVectorizer(vocabulary=new_vocab)\n",
    "#     tfidf_matrix = vectorizer.fit_transform([lemmatized_content])\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "#     corpus_index = [n for n in ['Values']]\n",
    "#     df = pd.DataFrame(tfidf_matrix.T.todense(), index=feature_names, columns=corpus_index)\n",
    "#     df = df.sort_values(by=['Values'], ascending=False)\n",
    "#     print (df.head(5).index.values.tolist())\n",
    "#     common_vocabularly_lem.update(df.head(1000).index.values.tolist())\n",
    "    common_vocabularly_lem.update(new_vocab)\n",
    "# not removed by spacy.    \n",
    "common_vocabularly_lem.remove('million')\n",
    "common_vocabularly_lem.remove('tausend')\n",
    "common_vocabularly_lem.remove('eur')\n",
    "common_vocabularly_lem.remove('teur')\n",
    "common_vocabularly_lem.remove('*')\n",
    "common_vocabularly_lem.remove('+')\n",
    "common_vocabularly_lem.remove('&')\n",
    "common_vocabularly_lem.remove('%')\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    glob_set = set()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        l = []\n",
    "        #print(glob_set)\n",
    "        for i in topic.argsort()[:-100 - 1:-1]:\n",
    "            if len(l) == n_top_words:\n",
    "                break\n",
    "            if feature_names[i] not in glob_set:\n",
    "                glob_set.add(feature_names[i])\n",
    "                l.append(feature_names[i])\n",
    "        message += \" \".join(l)\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_vocabularly_lem.remove('bmw')\n",
    "#common_vocabularly_lem.remove('group')\n",
    "#common_vocabularly_lem.remove('carl')\n",
    "#common_vocabularly_lem.remove('zeiss')\n",
    "#common_vocabularly_lem.remove('borussia')\n",
    "#common_vocabularly_lem.remove('dortmund')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(vocabulary=common_vocabularly_lem)\n",
    "tf = tf_vectorizer.fit_transform(lemm_docs_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=25, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=25,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0,\n",
    "                                   max_iter=50)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: � fraport-spezifischer fremd- fremdanteile fremdanteilen fremdbesitz fremdbezug fremdbezugsteile fremddarlehen fremddruck-\n",
      "Topic #1: evt bionamics innovate euprotec execute earn janssen gegenleistung hyperion vergleichsperiode\n",
      "Topic #2: fremddruckleistungen fremdeinwirkungen fremdfertigungsquote fremdfi fremdfinan- fremdfinanzie- fremdfinanzierung fremdfinanzierungen fremdfinanzierungs- fremdfinanzierungsangebot\n",
      "Topic #3: bayer vorjahr sondereinflüssen umsatz cropscience usa ebitda healthcare ebit wpb\n",
      "Topic #4: fremdfinanzierungsanteil fremdfinanzierungsbasis fremdfinanzierungsbedarf fremd frem- freizügigkeitsleis- freital freistaat freiste- freistehend\n",
      "Topic #5: aktie euro unternehmen bilfinger gesellschaft höhe konzern aufsichtsrat prozent vorstehen\n",
      "Topic #6: freistehende freistellung freistellungs- freistellungserklärungen freistellungsphase freitag freiverkehr freizeitphase freiwerdende freiwillig\n",
      "Topic #7: evonik aktivität bereinigen fortgeführt ergebnis operativ vivawest zusammenhang ertrag ionen\n",
      "Topic #8: milliarde segment quartal commerzbank bank bayernlb halbjahr deutlich insbesondere juni\n",
      "Topic #9: freiwillige freizeit freizeitangebote freizeitangeboten freizeitbeschäfti- freizeitfahrzeugen freizeitgestaltung fremdfinanzierungskonditionen fremdfinanzierungskosten fremdfinanzie­\n",
      "Topic #10: fire amadeus personalvermittlung zeitarbeit weiterbildung arbeitsmarkt ba qualifizieren berichtsquartal zuletzt\n",
      "Topic #11: hoch risiko mein entwicklung geschäftsjahr wesentlich beziehungsweise mitarbeiter basf anteil\n",
      "Topic #12: aixtron dezember se kunde led aufsichtsrats mio usd technologie rahmen\n",
      "Topic #13: bechtle it tsd service ifrs steuer berichtsjahr managed aufgrund markt\n",
      "Topic #14: covestro absatz stimmrechten stimmrechte oktober lagebericht 6wphg schwelle blackrock august\n",
      "Topic #15: evotec allianz bmw group bereich evotecs management mitglied hinaus mrd\n",
      "Topic #16: adva optical networking vermögenswerte net netz fsp übertragungstechnik cloud overture\n",
      "Topic #17: continental vj division adidas kredit contitech syndizierten positiv vergleichen reife\n",
      "Topic #18: monat september wachstum märz anstieg capital deutschland geschäftsjahres compugroup q1\n",
      "Topic #19: deutschen börse gruppe dzbank eurex clearstream ver clearing verlust verbindlichkeit\n",
      "Topic #20: fraport flughafen frankfurt passagier standort berichtszeitraum januar gesellschaften sonstig retail\n",
      "Topic #21: enbw energie strom adjusted baden württemberg künftig konzerns erneuerbaren erzeugung\n",
      "Topic #22: cewe online fotofinishing foto 4mio einzelhandel druck 5mio 2mio biotest\n",
      "Topic #23: bertrandt gmbh porsche abs stuttgart salzburg sitz holding aktiengesellschaft gemäß\n",
      "Topic #24: pfandbriefbank pbb value ias fms position credit wertmanagement hre kapital\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print (print_top_words(lda, tf_feature_names, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_rep_top_words(model, feature_names, n_top_words):\n",
    "    #matr = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        \n",
    "        message += \" \".join([str(feature_names[i]) + \": \" + \"{:.5f}\".format(model.components_[topic_idx, i])\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: �: 0.04000 fraport-spezifischer: 0.04000 fremd-: 0.04000 fremdanteile: 0.04000 fremdanteilen: 0.04000 fremdbesitz: 0.04000 fremdbezug: 0.04000 fremdbezugsteile: 0.04000 fremddarlehen: 0.04000 fremddruck-: 0.04000\n",
      "Topic #1: evt: 30.58348 bionamics: 21.96573 innovate: 21.10578 euprotec: 20.04000 execute: 19.33106 earn: 16.09364 janssen: 11.01888 gegenleistung: 10.94376 hyperion: 9.04000 vergleichsperiode: 9.03042\n",
      "Topic #2: �: 0.04000 fraport-spezifischer: 0.04000 fremd-: 0.04000 fremdanteile: 0.04000 fremdanteilen: 0.04000 fremdbesitz: 0.04000 fremdbezug: 0.04000 fremdbezugsteile: 0.04000 fremddarlehen: 0.04000 fremddruck-: 0.04000\n",
      "Topic #3: bayer: 242.47646 vorjahr: 136.19577 sondereinflüssen: 82.15943 umsatz: 70.88890 cropscience: 61.03360 usa: 60.59338 ebitda: 58.25339 healthcare: 52.58574 ebit: 51.87215 wpb: 49.04000\n",
      "Topic #4: �: 0.04000 fraport-spezifischer: 0.04000 fremd-: 0.04000 fremdanteile: 0.04000 fremdanteilen: 0.04000 fremdbesitz: 0.04000 fremdbezug: 0.04000 fremdbezugsteile: 0.04000 fremddarlehen: 0.04000 fremddruck-: 0.04000\n",
      "Topic #5: aktie: 965.17974 euro: 934.11421 vorjahr: 866.84923 unternehmen: 829.97124 bilfinger: 804.04000 gesellschaft: 783.82140 höhe: 755.03123 konzern: 742.99679 aufsichtsrat: 695.17682 prozent: 687.48398\n",
      "Topic #6: �: 0.04000 fraport-spezifischer: 0.04000 fremd-: 0.04000 fremdanteile: 0.04000 fremdanteilen: 0.04000 fremdbesitz: 0.04000 fremdbezug: 0.04000 fremdbezugsteile: 0.04000 fremddarlehen: 0.04000 fremddruck-: 0.04000\n",
      "Topic #7: evonik: 123.48462 aktivität: 67.26270 bereinigen: 62.25903 fortgeführt: 53.24882 ebitda: 35.16436 ergebnis: 34.85237 operativ: 28.64550 vivawest: 28.04000 zusammenhang: 27.21104 ertrag: 27.08303\n",
      "Topic #8: euro: 1342.48050 milliarde: 492.03307 segment: 461.08776 ergebnis: 360.21463 quartal: 326.71168 commerzbank: 260.96556 bank: 241.75881 vorjahr: 229.36383 bayernlb: 210.04000 halbjahr: 191.29830\n",
      "Topic #9: �: 0.04000 fraport-spezifischer: 0.04000 fremd-: 0.04000 fremdanteile: 0.04000 fremdanteilen: 0.04000 fremdbesitz: 0.04000 fremdbezug: 0.04000 fremdbezugsteile: 0.04000 fremddarlehen: 0.04000 fremddruck-: 0.04000\n",
      "Topic #10: fire: 14.04000 amadeus: 14.04000 personalvermittlung: 8.04000 zeitarbeit: 7.02241 weiterbildung: 6.49655 arbeitsmarkt: 5.65204 ba: 4.93805 qualifizieren: 4.74578 berichtsquartal: 3.90380 zuletzt: 3.20880\n",
      "Topic #11: vorjahr: 1600.25601 konzern: 1446.57266 höhe: 1374.08458 unternehmen: 1206.94991 hoch: 1111.51544 risiko: 1013.27029 ergebnis: 1010.41835 mein: 1000.35296 entwicklung: 988.38844 geschäftsjahr: 929.47624\n",
      "Topic #12: aixtron: 1181.04000 gesellschaft: 386.94306 dezember: 312.32764 vorstehen: 261.36506 aufsichtsrat: 258.94572 se: 232.13524 geschäftsjahr: 230.44368 konzern: 224.46322 unternehmen: 193.71289 entwicklung: 186.34951\n",
      "Topic #13: bechtle: 861.04000 prozent: 430.36204 it: 424.51831 vorjahr: 317.83307 tsd: 306.04000 unternehmen: 248.10055 risiko: 223.05957 konzern: 211.56837 segment: 181.37587 mio: 173.13814\n",
      "Topic #14: covestro: 80.03268 absatz: 31.95032 bayer: 25.72969 stimmrechten: 20.37168 stimmrechte: 18.77358 oktober: 13.83576 lagebericht: 10.87594 6wphg: 9.86818 schwelle: 9.73928 blackrock: 9.66285\n",
      "Topic #15: evotec: 1346.04000 allianz: 1117.23191 unternehmen: 932.64821 konzern: 885.90493 bmw: 828.82854 risiko: 666.98522 höhe: 643.56843 dezember: 603.45511 entwicklung: 564.02955 gesellschaft: 547.28608\n",
      "Topic #16: adva: 520.04000 optical: 498.04000 networking: 456.02137 konzern: 234.11807 unternehmen: 132.99316 dezember: 97.39968 höhe: 78.76218 vermögenswerte: 71.67095 kunde: 65.82743 net: 61.31757\n",
      "Topic #17: mio: 1365.10078 continental: 654.04000 vj: 442.12370 höhe: 245.21829 division: 191.20719 adidas: 142.26418 kredit: 120.67295 contitech: 110.98089 umsatz: 109.12908 usa: 108.16132\n",
      "Topic #18: quartal: 1624.80202 umsatz: 643.16494 monat: 639.76541 vorjahr: 480.53630 konzern: 404.00047 september: 389.09918 höhe: 379.77174 wachstum: 361.19712 euro: 351.19829 vergleichen: 345.37670\n",
      "Topic #19: deutschen: 990.78861 börse: 967.22392 gruppe: 952.24149 dzbank: 842.04000 risiko: 672.85839 höhe: 549.36159 unternehmen: 507.94106 vorjahr: 459.07271 eurex: 376.04000 clearstream: 360.04000\n",
      "Topic #20: mio: 1445.79559 fraport: 693.04000 konzern: 437.03614 vorjahr: 314.63114 flughafen: 229.03917 frankfurt: 199.30721 höhe: 120.62659 geschäftsjahr: 115.73197 umsatz: 114.59486 aufgrund: 103.65041\n",
      "Topic #21: enbw: 1644.04000 energie: 400.70354 strom: 311.73529 vorjahr: 216.82902 adjusted: 189.82733 unternehmen: 181.88126 konzern: 178.52838 bereich: 178.16297 deutlich: 173.68653 netz: 169.28011\n",
      "Topic #22: euro: 388.46906 cewe: 305.04000 online: 93.37438 fotofinishing: 73.04000 foto: 61.04000 quartal: 60.88945 umsatz: 49.55605 4mio: 48.20510 einzelhandel: 45.91646 druck: 41.98819\n",
      "Topic #23: bertrandt: 427.04000 prozent: 344.13570 gmbh: 248.61120 porsche: 208.04000 vorjahr: 193.30015 abs: 121.76909 konzern: 99.73583 stuttgart: 94.02399 salzburg: 86.04000 sitz: 79.83836\n",
      "Topic #24: konzern: 415.88044 pfandbriefbank: 334.04000 milliarde: 328.93613 pbb: 285.04000 deutschen: 282.19007 dezember: 243.12025 risiko: 154.06185 value: 123.37438 höhe: 117.95944 ias: 115.90432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_with_rep_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
