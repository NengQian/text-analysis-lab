{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlab_LSI_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds, eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py\n",
    "%run 'load_and_prepro_document.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI\n",
    "LSI is actually just doing SVD at TF-IDF matrix, and to get an approximate TF-IDF matrix with low number of dimension.\n",
    "Here I try to use LSI to realize a information retrieval application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here just use os lib to get the 1000 documents in this folder\n",
    "import os\n",
    "documents_list = list()\n",
    "for root, dirs, files in os.walk(\"./LabShare/data/all/json\", topdown=False):\n",
    "    for name in files:\n",
    "        documents_list.append(name)\n",
    "documents_list = documents_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5903236865997314\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# here I override the preProcess() in fit_transform(). Because the input data is already preprocessed.\n",
    "def preProcess(s):\n",
    "    return s\n",
    "my_doc, my_doc_name = get_clean_data(documents_list)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform( my_doc )\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search query is: \n",
      "{'sportbekleidung', 'schuh'}\n",
      "------------------------------------\n",
      "related document: \t related score\n",
      "PUMA-QuarterlyReport-2012-Q3.json:\t0.9705766715598944\n",
      "PUMA-QuarterlyReport-2012-Q2.json:\t0.9584773368670201\n",
      "PUMA-QuarterlyReport-2015-Q1.json:\t0.9543901309787146\n",
      "PUMA-QuarterlyReport-2014-Q3.json:\t0.9507635544600765\n",
      "PUMA-QuarterlyReport-2010-Q2.json:\t0.9435183542968074\n",
      "PUMA-QuarterlyReport-2011-Q3.json:\t0.9304272544834216\n",
      "PUMA-QuarterlyReport-2010-Q1.json:\t0.9128649922630564\n",
      "PUMA-AnnualReport-2013.json:\t0.8812087556023984\n",
      "Adidas-AnnualReport-2016.json:\t0.3034457396028766\n",
      "Zalando-AnnualReport-2015.json:\t0.21861967178319974\n"
     ]
    }
   ],
   "source": [
    "# now compute the input query's vector.\n",
    "query = 'Sportbekleidung schuh '   #query string\n",
    "\n",
    "# step 1, do same preprosseing for this query\n",
    "nlp = spacy.load(\"de\")\n",
    "sentence = nlp(query, disable=['parser', 'ner'])\n",
    "filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "filtered_words_withoutpunc = [word for word in filtered_words_withoutdigits if word.pos_ != 'PUNCT']\n",
    "filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "\n",
    "vocabularly = set()\n",
    "for word in filtered_lemmas:\n",
    "    vocabularly.add(word.replace('\\n', '').strip().lower())\n",
    "\n",
    "new_vocab = set()\n",
    "for u in vocabularly:\n",
    "    if u != '':\n",
    "        new_vocab.add(u)\n",
    "\n",
    "# step 2, generate query's tf-idf vector\n",
    "query_vector_ori = np.zeros(tfidf_matrix.shape[1]) #initilize the query vector\n",
    "idf = vectorizer.idf_\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "# find my words in this feature_name list, and its corresponding index\n",
    "print(\"search query is: \")\n",
    "print(new_vocab)\n",
    "for words in new_vocab:\n",
    "    idx = feature_name.index(words)\n",
    "    query_vector_ori[idx] = idf[idx]\n",
    "# do normalize\n",
    "query_vector_ori = query_vector_ori/np.linalg.norm(query_vector_ori)\n",
    "\n",
    "# step3, transfer the origin vector to low_dim space\n",
    "k = 100\n",
    "u, s, vt = svds(tfidf_matrix.T, k=k)  # transpose the tfidf_matrix, get item*document\n",
    "#here k is the remaining dimension. could from 1 to (number of document-1)\n",
    "# d_hat = s.inv*U.t*d    \n",
    "s_dig = np.diag(s)\n",
    "query_vector_low_dim = ((np.linalg.inv(s_dig)).dot(u.T)).dot(query_vector_ori)\n",
    "# get query in low dim\n",
    "\n",
    "# step4, compute the similarity\n",
    "def calculate_simility(q1,q2):\n",
    "    sim = q1.dot(q2)/(np.linalg.norm(q1)*np.linalg.norm(q2))\n",
    "    return sim\n",
    "sim = np.zeros(vt.shape[1])\n",
    "for i in range(0,vt.shape[1]):\n",
    "    sim[i] = calculate_simility(query_vector_low_dim,vt[:,i])\n",
    "\n",
    "# step5, take top 10 similar document\n",
    "top_idx = np.argsort(-sim)[0:10]  # here -sim, since I want t get decending order sort,and get the top 3 index\n",
    "print('------------------------------------')\n",
    "print('related document: \\t related score')\n",
    "for i in top_idx:\n",
    "    print(my_doc_name[i]+':\\t'+ str(sim[i]))\n",
    "\n",
    "# try to find some way to connect document and this index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
