{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlab_svd_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds, eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_test = ['BMW-AnnualReport-2016.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_documents = ['BMW-AnnualReport-2016.json', 'CarlZeissMeditec-AnnualReport-2016.json', 'BVB-AnnualReport-2016.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['BMW-AnnualReport-2015.json', \n",
    "             'BMW-AnnualReport-2016.json', \n",
    "             'BMW-AnnualReport-2017.json', \n",
    "             'CarlZeissMeditec-AnnualReport-2015.json', \n",
    "             'CarlZeissMeditec-AnnualReport-2016.json', \n",
    "             'CarlZeissMeditec-AnnualReport-2017.json',\n",
    "             'BVB-AnnualReport-2015.json', \n",
    "             'BVB-AnnualReport-2016.json', \n",
    "             'BVB-AnnualReport-2017.json',\n",
    "             'Aareal-AnnualReport-2010.json',\n",
    "             'Adidas-AnnualReport-2010.json',\n",
    "             'AdlerRealEstate-AnnualReport-2014.json',\n",
    "             'ADOProperties-QuarterlyReport-2017-Q1.json',\n",
    "             'Airbus-AnnualReport-2012.json',\n",
    "             'Aixtron-AnnualReport-2016.json',\n",
    "             'Allianz-AnnualReport-2015.json',\n",
    "             'alstria-AnnualReport-2012.json',\n",
    "             'AmadeusFiRe-AnnualReport-2013.json'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = 'type'\n",
    "PARAGRAPH = 'paragraph'\n",
    "CONTENT = 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readContentOfFile(file_name):\n",
    "    content = ''\n",
    "    try:\n",
    "        with open(file_name) as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                typeDoc = item[TYPE]\n",
    "                if typeDoc == PARAGRAPH:\n",
    "                    content += item[CONTENT]\n",
    "    except:\n",
    "        FileUtils.fix_json(file_name)\n",
    "        with open(file_name) as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                typeDoc = item[TYPE]\n",
    "                if typeDoc == PARAGRAPH:\n",
    "                    content += item[CONTENT]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given documents list, to upload the current after preprocessing document and vocabularly\n",
    "# input parameter:\n",
    "#    documents: a list, contain documents' name\n",
    "# output:\n",
    "#    document_prepro: a list, contain strings, which contain after preprocessing input document's content.\n",
    "#    vocabularly: a set, contains vocabularly we get from input document list\n",
    "def update_preprocess_data_and_vocabularly(documents):\n",
    "    vocabularly = set()\n",
    "    document_prepro = list()\n",
    "    nlp = spacy.load(\"de\")\n",
    "    for document in documents:\n",
    "        content_of_document = readContentOfFile(FILE_PATH+document)\n",
    "        sentence = nlp(content_of_document)\n",
    "        filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "        filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "        filtered_words_withoutcurrency = [word for word in filtered_words_withoutdigits if not word.is_currency]\n",
    "        filtered_words_withoutverbs = [word for word in filtered_words_withoutcurrency if word.pos_ != 'VERB']\n",
    "        filtered_words_withoutnum = [word for word in filtered_words_withoutverbs if word.pos_ != 'NUM']\n",
    "        filtered_words_withoutsym = [word for word in filtered_words_withoutnum if word.pos_ != 'SYM']\n",
    "        filtered_words_withoutpunc = [word for word in filtered_words_withoutsym if word.pos_ != 'PUNCT']\n",
    "        filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "        tmp = set()\n",
    "        for word in filtered_lemmas:\n",
    "            tmp.add(word.replace('\\n', '').strip().lower())\n",
    "        new_vocab = set()\n",
    "        for u in tmp:\n",
    "            if u != '':\n",
    "                new_vocab.add(u)\n",
    "        vocabularly.update(new_vocab)\n",
    "        lemmatized_content = \" \".join(item for item in filtered_lemmas)\n",
    "        document_prepro.append(lemmatized_content.lower())\n",
    "       \n",
    "    vocabularly.remove('million')\n",
    "    vocabularly.remove('tausend')\n",
    "    vocabularly.remove('eur')\n",
    "    vocabularly.remove('teur')\n",
    "    vocabularly.remove('*')\n",
    "    vocabularly.remove('+')\n",
    "    vocabularly.remove('&')\n",
    "    vocabularly.remove('%')\n",
    "    return vocabularly, document_prepro\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.49612021446228\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "my_voc, my_doc = update_preprocess_data_and_vocabularly(documents)\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=my_voc)\n",
    "tfidf_matrix = vectorizer.fit_transform( my_doc)\n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kranken', 'arzt'}\n",
      "2.3350010667323398\n",
      "19003\n",
      "2.55814461804655\n",
      "3214\n",
      "[-0.14607688 -0.14500215 -0.14500215  0.99172208  0.99237067  0.99168017\n",
      " -0.15109107 -0.16220985 -0.14787144  0.53310718  0.63828439  0.50989149\n",
      "  0.59832618  0.54703209  0.57013083  0.62034624  0.53400373  0.82639627]\n",
      "[4 3 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# now compute the input query's vector.\n",
    "query = \"europe \"  #query string\n",
    "\n",
    "\n",
    "# step 1, do preprosseing for this query\n",
    "nlp = spacy.load(\"de\")\n",
    "sentence = nlp(query)\n",
    "filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS]\n",
    "filtered_words_withoutdigits = [word for word in filtered_words if not word.is_digit]\n",
    "filtered_words_withoutpunc = [word for word in filtered_words_withoutdigits if word.pos_ != 'PUNCT']\n",
    "filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "vocabularly = set()\n",
    "for word in filtered_lemmas:\n",
    "    vocabularly.add(word.replace('\\n', '').strip().lower())\n",
    "new_vocab = set()\n",
    "for u in vocabularly:\n",
    "    if u != '':\n",
    "        new_vocab.add(u)\n",
    "        \n",
    "\n",
    "# step 2, generate query's tf-idf vector\n",
    "query_vector_ori = np.zeros(tfidf_matrix.shape[1]) #initilize the query vector\n",
    "idf = vectorizer.idf_\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "#print(idf)\n",
    "# find my words in this feature_name list, and its corresponding index\n",
    "print(new_vocab)\n",
    "for words in new_vocab:\n",
    "    idx = feature_name.index(words)\n",
    "    query_vector_ori[idx] = idf[idx]\n",
    "    print(query_vector_ori[idx])\n",
    "    print(idx)\n",
    "# do normalize\n",
    "query_vector_ori = query_vector_ori/np.linalg.norm(query_vector_ori)\n",
    "    \n",
    "# step3, transfer the origin vector to low_dim space\n",
    "k = 3\n",
    "u, s, vt = svds(tfidf_matrix.T, k=k)  # transpose the tfidf_matrix, get item*document\n",
    "#here k is the remaining dimension. could from 1 to (number of document-1), could try take all paragraph as document\n",
    "# k could be a parameter\n",
    "# d_hat = s.inv*U.t*d    may be not right?\n",
    "s_dig = np.diag(s)\n",
    "query_vector_low_dim = ((np.linalg.inv(s_dig)).dot(u.T)).dot(query_vector_ori)\n",
    "# get Vk\n",
    "#tfidf_matrix_low_dim = (u.dot(s_dig)).dot(vt)\n",
    "\n",
    "# step4, compute the similarity\n",
    "def calculate_simility(q1,q2):\n",
    "    sim = q1.dot(q2)/(np.linalg.norm(q1)*np.linalg.norm(q2))\n",
    "    return sim\n",
    "sim = np.zeros(vt.shape[1])\n",
    "for i in range(0,vt.shape[1]):\n",
    "    sim[i] = calculate_simility(query_vector_low_dim,vt[:,i])\n",
    "\n",
    "# step5, take top 3 similar document\n",
    "print(sim)\n",
    "top_3_idx = np.argsort(-sim)[0:3]  # here -sim, since I want t get decending order sort,and get the top 3 index\n",
    "print(top_3_idx)\n",
    "# try to find some way to connect document and this index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'auto' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8017d57c2fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'auto' is not in list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5021"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
