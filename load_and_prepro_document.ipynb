{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess fild and load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/file_utils.py\n",
    "%run src/configuration.py   #store useful file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = 'type'\n",
    "PARAGRAPH = 'paragraph'\n",
    "CONTENT = 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read non german file list\n",
    "# we have a Non_german_file list file, so here we just read this list from that txt file.\n",
    "# this list will be used to filter all non german files\n",
    "with open(NON_GERMAN_FILE_PATH) as f:\n",
    "    non_german_documents = f.readlines()\n",
    "non_german_documents = [x.strip() for x in non_german_documents] \n",
    "temp_docu = list()\n",
    "for document in non_german_documents:\n",
    "    temp_docu.append(document + '.json')\n",
    "non_german_documents = temp_docu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciton to read raw json file\n",
    "def readContentOfParagraphs(file_name):\n",
    "    contents = []\n",
    "    try:\n",
    "        data = json.loads(FileUtils.fix_json(file_name))\n",
    "        for item in data:\n",
    "            typeDoc = item[TYPE]\n",
    "            if typeDoc == PARAGRAPH:\n",
    "                contents.append(item[CONTENT])\n",
    "    except:\n",
    "        print('Bad file: ' + file_name)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we using spacy to do preprocessing, remove stop words, useless words and so on\n",
    "def lemmatize_paragraphs(document):\n",
    "    nlp = spacy.load(\"de\",disable=['parser', 'ner'])\n",
    "    paragraphs = readContentOfParagraphs(document)\n",
    "    # here is try to remove company name in documents, becasue these name is useless and harmful for our approach\n",
    "    company_name = document[len(FILE_PATH):document.find('-')].lower()\n",
    "    lemmatized_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        # remove the - in document\n",
    "        content_of_document = paragraph.replace('-\\n','')\n",
    "        content_of_document = content_of_document.replace('\\n',' ')\n",
    "    \n",
    "        #replace all gco2 with co2\n",
    "        content_of_document = content_of_document.replace('gCO2','CO2')\n",
    "    \n",
    "        #remove the character we don't need\n",
    "        remove_char = content_of_document.maketrans('-',' ','+*<>%/&$')\n",
    "        content_of_document = content_of_document.translate(remove_char)\n",
    "    \n",
    "        sentence = nlp(content_of_document)\n",
    "        filtered_words = [word for word in sentence if word.lower_ not in STOP_WORDS] \n",
    "        filtered_words_withoutnum = [word for word in filtered_words if word.pos_ != 'NUM']\n",
    "        filtered_words_withoutsym = [word for word in filtered_words_withoutnum if word.pos_ != 'SYM']\n",
    "        filtered_words_withoutdigits = [word for word in filtered_words_withoutsym if not word.is_digit]\n",
    "        filtered_words_withoutpunc = [word for word in filtered_words_withoutdigits if word.pos_ != 'PUNCT']\n",
    "        filtered_lemmas = [word.lemma_ for word in filtered_words_withoutpunc]\n",
    "    \n",
    "        final = []  \n",
    "        for item in filtered_lemmas:\n",
    "            #remove the words contain digit except of co2\n",
    "            if company_name in item.lower():\n",
    "                continue\n",
    "                \n",
    "            if(any(c.isdigit() for c in item)):\n",
    "                if 'CO2' in item:\n",
    "                    final.append(item)\n",
    "            else:\n",
    "                #remove the words contain dot\n",
    "                if '.' not in item:\n",
    "                    final.append(item)\n",
    "        \n",
    "        lemmatized_content = \" \".join(item for item in final)\n",
    "        lemmatized_paragraphs.append(lemmatized_content.lower())\n",
    "        \n",
    "    #output the result into file \n",
    "    if document.startswith(FILE_PATH):# if file path is in document name, remove it\n",
    "        filename = CLEAN_FILE_PREFIX  + document[len(FILE_PATH):]\n",
    "    filename = SHARE_SPACE_FOLDER_PATH + filename  # add the clean data folder path\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(lemmatized_paragraphs, outfile) # save clean data in json file, which is easy to read\n",
    "        \n",
    "    return lemmatized_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function try to load clean paragraph which we already have done preprocess and saved in folder\n",
    "def load_lemmatization_paragraph(document):\n",
    "    with open(document, 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "    return datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter :\n",
    "#     documents_list: a list, contain string of  file name, which we want to preprocessing\n",
    "#     get_paragraph: True if you want to get every paragrah, False if you want to get the whole document\n",
    "#     logging: default = False, if set it as true, then it will print information about which document are currently preprocessing\n",
    "#     and which document has been already done.\n",
    "# return :\n",
    "#     documents_clean: a list contain string, which contain the whole document\n",
    "#     documents_clean_name: a list contain string, every output documents' corresponding name\n",
    "\n",
    "def get_clean_data(documents_list, get_paragraph = False, logging=False):\n",
    "    documents_clean = list()\n",
    "    documents_clean_name = list()\n",
    "\n",
    "    for document in documents_list:\n",
    "        # fist check if this document is english\n",
    "        if document in non_german_documents:\n",
    "            if logging:\n",
    "                print(\"this file \"+ document + ' is non german, skip it')\n",
    "            continue \n",
    "        # second check if this doc already be preprocessed\n",
    "        my_file = Path(SHARE_SPACE_FOLDER_PATH + CLEAN_FILE_PREFIX + document)\n",
    "        if my_file.is_file():\n",
    "        # if already exist, we directly load the clean data from hard disk\n",
    "            if logging: \n",
    "                print(CLEAN_FILE_PREFIX + document + \" has already done preprocess\")\n",
    "            # load file!\n",
    "            documents_clean_name.append(document)\n",
    "            documents_clean.append(load_lemmatization_paragraph(SHARE_SPACE_FOLDER_PATH + CLEAN_FILE_PREFIX + document))\n",
    "        else:\n",
    "        # if not find, we do preprocess for this document, and save it in hard disk\n",
    "            documents_clean_name.append(document)\n",
    "            documents_clean.append(lemmatize_paragraphs(FILE_PATH + document))\n",
    "\n",
    "    if not get_paragraph:\n",
    "        # if we don't want paragraph, but the whole report. Here we join all paragraph to get a whole report. \n",
    "        documents_tmp = list()\n",
    "        for document in documents_clean:\n",
    "            document_tmp = \" \".join(para for para in document)\n",
    "            documents_tmp.append(document_tmp)\n",
    "        documents_clean = documents_tmp\n",
    "    return documents_clean, documents_clean_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
